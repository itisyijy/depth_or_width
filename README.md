# Depth vs Width: ResNet vs Wide Residual Networks λΉ„κµ μ—°κµ¬

μ΄ ν”„λ΅μ νΈλ” λ”¥λ¬λ‹μ—μ„ λ„¤νΈμ›ν¬μ **κΉμ΄(Depth)**μ™€ **λ„λΉ„(Width)** μ¤‘ μ–΄λ κ²ƒμ΄ μ„±λ¥ ν–¥μƒμ— λ” ν¨μ¨μ μΈμ§€ λΉ„κµ λ¶„μ„ν•λ” μ—°κµ¬μ…λ‹λ‹¤. ResNet-110 (Deep & Thin)κ³Ό WRN-28-2 (Wide & Shallow) λ¨λΈμ„ CIFAR-10 λ°μ΄ν„°μ…‹μ—μ„ ν•™μµν•κ³  ν‰κ°€ν•©λ‹λ‹¤.

## π“‹ λ©μ°¨

- [ν”„λ΅μ νΈ κ°μ”](#ν”„λ΅μ νΈ-κ°μ”)
- [μ£Όμ” νΉμ§•](#μ£Όμ”-νΉμ§•)
- [μ„¤μΉ λ°©λ²•](#μ„¤μΉ-λ°©λ²•)
- [μ‚¬μ© λ°©λ²•](#μ‚¬μ©-λ°©λ²•)
- [ν”„λ΅μ νΈ κµ¬μ΅°](#ν”„λ΅μ νΈ-κµ¬μ΅°)
- [μ‹¤ν— κ²°κ³Ό](#μ‹¤ν—-κ²°κ³Ό)
- [μ°Έκ³  λ¬Έν—](#μ°Έκ³ -λ¬Έν—)

## π― ν”„λ΅μ νΈ κ°μ”

μ΄ ν”„λ΅μ νΈλ” λ‹¤μ μ—°κµ¬ μ§λ¬Έμ— λ‹µν•κΈ° μ„ν•΄ μ„¤κ³„λμ—μµλ‹λ‹¤:

> **"CNN μ„±λ¥ ν–¥μƒμ„ μ„ν•΄ λ¨λΈμ κΉμ΄(Depth)μ™€ λ„λΉ„(Width) μ¤‘ μ–΄λ κ²ƒμ΄ λ” ν¨μ¨μ μΈκ°€?"**

### λΉ„κµ λ¨λΈ

1. **ResNet-110** (Baseline)
   - Deep & Thin κµ¬μ΅°
   - 110 layers, widening factor k=1
   - μ•½ 1.73M νλΌλ―Έν„°

2. **WRN-28-2**
   - Wide & Shallow κµ¬μ΅°
   - 28 layers, widening factor k=2
   - μ•½ 1.47M νλΌλ―Έν„°

3. **WRN-28-2-Dropout**
   - WRN-28-2μ— Dropout(0.3) μ μ©
   - μΌλ°ν™” μ„±λ¥ ν–¥μƒμ„ μ„ν• λ³€ν• λ¨λΈ

## β¨ μ£Όμ” νΉμ§•

- **κ³µμ •ν• λΉ„κµ**: μ μ‚¬ν• νλΌλ―Έν„° μμ‚°(μ•½ 1.5M~1.7M) λ‚΄μ—μ„ λ¨λΈ λΉ„κµ
- **ν‘μ¤€ν™”λ μ‹¤ν— μ„¤μ •**: WRN λ…Όλ¬Έμ ν‘μ¤€ ν•μ΄νΌνλΌλ―Έν„° μ‚¬μ©
- **μ™„μ „ν• μ¬ν„μ„±**: κ³ μ •λ random seed(42) μ‚¬μ©
- **μƒμ„Έν• λ¶„μ„**: ν•™μµ κ³΅μ„ , ν΄λμ¤λ³„ μ„±λ¥, ν¨μ¨μ„± λ¶„μ„ ν¬ν•¨

## π€ μ„¤μΉ λ°©λ²•

### μ”κµ¬μ‚¬ν•­

- Python 3.7 μ΄μƒ
- CUDA μ§€μ› GPU (κ¶μ¥, CPUλ„ κ°€λ¥ν•μ§€λ§ λλ¦Ό)

### 1. μ €μ¥μ† ν΄λ΅ 

```bash
git clone <repository-url>
cd WRN
```

### 2. κ°€μƒν™κ²½ μƒμ„± λ° ν™μ„±ν™” (κ¶μ¥)

```bash
# κ°€μƒν™κ²½ μƒμ„±
python -m venv venv

# κ°€μƒν™κ²½ ν™μ„±ν™”
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate
```

### 3. μμ΅΄μ„± μ„¤μΉ

```bash
pip install -r requirements.txt
```

### 4. GPU μ§€μ› ν™•μΈ (μ„ νƒμ‚¬ν•­)

PyTorchκ°€ GPUλ¥Ό μΈμ‹ν•λ”μ§€ ν™•μΈ:

```python
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}")
```

## π“– μ‚¬μ© λ°©λ²•

### Jupyter Notebook μ‹¤ν–‰

1. Jupyter Notebook μ‹¤ν–‰:

```bash
jupyter notebook
```

2. `depth_or_width.ipynb` νμΌμ„ μ—΄κ³  μμ„λ€λ΅ μ…€μ„ μ‹¤ν–‰ν•©λ‹λ‹¤.

### λ…ΈνΈλ¶ κµ¬μ΅°

λ…ΈνΈλ¶μ€ λ‹¤μ μ„Ήμ…μΌλ΅ κµ¬μ„±λμ–΄ μμµλ‹λ‹¤:

1. **Problem Definition**: ResNetκ³Ό WRNμ μ΄λ΅ μ  λ°°κ²½
2. **Dataset**: CIFAR-10 λ°μ΄ν„°μ…‹ λ΅λ“ λ° μ „μ²λ¦¬
3. **Model Design & Implementation**: 
   - ResNet-110 κµ¬ν„
   - WRN-28-2 κµ¬ν„
4. **Training**: λ¨λΈ ν•™μµ (200 epochs)
5. **Testing**: ν…μ¤νΈ μ„ΈνΈμ—μ„ ν‰κ°€
6. **Result Analysis**: 
   - ν•™μµ κ³΅μ„  μ‹κ°ν™”
   - μ„±λ¥ λΉ„κµ
   - ν΄λμ¤λ³„ μ„±λ¥ λ¶„μ„
   - ν¨μ¨μ„± λ¶„μ„

### ν•™μµλ λ¨λΈ μ‚¬μ©

ν•™μµμ΄ μ™„λ£λλ©΄ `checkpoints/` λ””λ ‰ν† λ¦¬μ— λ¨λΈμ΄ μ €μ¥λ©λ‹λ‹¤:

```
checkpoints/
β”β”€β”€ resnet110/
β”‚   β”β”€β”€ best_model.pth
β”‚   β”β”€β”€ final_model.pth
β”‚   β””β”€β”€ training_history.json
β”β”€β”€ wrn28_2/
β”‚   β”β”€β”€ best_model.pth
β”‚   β”β”€β”€ final_model.pth
β”‚   β””β”€β”€ training_history.json
β””β”€β”€ wrn28_2_dropout/
    β”β”€β”€ best_model.pth
    β”β”€β”€ final_model.pth
    β””β”€β”€ training_history.json
```

λ¨λΈ λ΅λ“ μμ‹:

```python
import torch
from model import ResNet110  # λλ” WRN28_2

# λ¨λΈ μΈμ¤ν„΄μ¤ μƒμ„±
model = ResNet110(num_classes=10)

# μ²΄ν¬ν¬μΈνΈ λ΅λ“
checkpoint = torch.load('./checkpoints/resnet110/best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
```

## π“ ν”„λ΅μ νΈ κµ¬μ΅°

```
WRN/
β”β”€β”€ depth_or_width.ipynb          # λ©”μΈ λ…ΈνΈλ¶ νμΌ
β”β”€β”€ requirements.txt               # Python ν¨ν‚¤μ§€ μμ΅΄μ„±
β”β”€β”€ README.md                      # ν”„λ΅μ νΈ μ„¤λ…μ„
β”β”€β”€ data/                          # CIFAR-10 λ°μ΄ν„°μ…‹ (μλ™ λ‹¤μ΄λ΅λ“)
β”‚   β””β”€β”€ cifar-10-batches-py/
β”β”€β”€ checkpoints/                   # ν•™μµλ λ¨λΈ μ²΄ν¬ν¬μΈνΈ
β”‚   β”β”€β”€ resnet110/
β”‚   β”β”€β”€ wrn28_2/
β”‚   β””β”€β”€ wrn28_2_dropout/
β””β”€β”€ test_results_summary.json      # ν…μ¤νΈ κ²°κ³Ό μ”μ•½
```

## π“ μ‹¤ν— κ²°κ³Ό

### μ£Όμ” κ²°κ³Ό μ”μ•½

| λ¨λΈ | νλΌλ―Έν„° μ | κΉμ΄ | λ„λΉ„(k) | ν…μ¤νΈ μ •ν™•λ„ | ν…μ¤νΈ Loss |
|------|------------|------|---------|--------------|-------------|
| ResNet-110 | 1,730,522 | 110 | 1 | 94.29% | 0.2656 |
| WRN-28-2 | 1,467,610 | 28 | 2 | 94.62% | 0.2151 |
| WRN-28-2-Dropout | 1,467,610 | 28 | 2 | **94.75%** | **0.1989** |

### μ£Όμ” λ°κ²¬

1. **λ„λΉ„ ν™•μ¥μ ν¨μ¨μ„±**: WRN-28-2λ” ResNet-110λ³΄λ‹¤ μ•½ 15% μ μ€ νλΌλ―Έν„°λ΅ λ” λ†’μ€ μ„±λ¥μ„ λ‹¬μ„±
2. **Dropoutμ ν¨κ³Ό**: WRN-28-2-Dropoutμ΄ κ°€μ¥ λ†’μ€ ν…μ¤νΈ μ •ν™•λ„μ™€ κ°€μ¥ λ‚®μ€ ν…μ¤νΈ Lossλ¥Ό κΈ°λ΅
3. **ν¨μ¨μ„±**: WRN μ‹λ¦¬μ¦ λ¨λΈμ΄ ResNet-110λ³΄λ‹¤ μ•½ 18% λ†’μ€ ν¨μ¨μ„±(μ •ν™•λ„/νλΌλ―Έν„°)μ„ λ³΄μ„

## π”§ ν•μ΄νΌνλΌλ―Έν„°

### ν•™μµ μ„¤μ •

- **Optimizer**: SGD with Momentum
- **Initial Learning Rate**: 0.1
- **Momentum**: 0.9
- **Weight Decay**: 0.0005
- **Learning Rate Schedule**: Multi-step decay
  - Epoch 60: 0.1 β†’ 0.02
  - Epoch 120: 0.02 β†’ 0.004
  - Epoch 160: 0.004 β†’ 0.0008
- **Batch Size**: 128
- **Total Epochs**: 200
- **Random Seed**: 42

### λ°μ΄ν„° μ¦κ°•

- **Training**: 4-pixel padding + Random horizontal flip + Random crop
- **Validation/Test**: Normalization only

## π› λ¬Έμ  ν•΄κ²°

### CUDA out of memory μ¤λ¥

λ°°μΉ ν¬κΈ°λ¥Ό μ¤„μ΄κ±°λ‚ GPU λ©”λ¨λ¦¬λ¥Ό ν™•μΈν•μ„Έμ”:

```python
# λ°°μΉ ν¬κΈ° μ΅°μ • (μ: 128 β†’ 64)
batch_size = 64
```

### λ°μ΄ν„°μ…‹ λ‹¤μ΄λ΅λ“ μ‹¤ν¨

μΈν„°λ„· μ—°κ²°μ„ ν™•μΈν•κ±°λ‚ μλ™μΌλ΅ CIFAR-10 λ°μ΄ν„°μ…‹μ„ λ‹¤μ΄λ΅λ“ν•μ—¬ `data/` λ””λ ‰ν† λ¦¬μ— λ°°μΉν•μ„Έμ”.

## π“ μ°Έκ³  λ¬Έν—

1. He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. [DOI](
https://doi.org/10.48550/arXiv.1512.03385)

2. Zagoruyko, Sergey, and Nikos Komodakis. "Wide Residual Networks." Proceedings of the British Machine Vision Conference (BMVC). 2016. [DOI]()
https://doi.org/10.48550/arXiv.1605.07146 [GitHub](https://github.com/szagoruyko/wide-residual-networks)

3. CIFAR-10 Dataset: https://www.cs.toronto.edu/~kriz/cifar.html
